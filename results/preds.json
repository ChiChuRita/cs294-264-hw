{
  "django__django-11179": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-11179",
    "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex 92fe4affb9..d7c9fb4755 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -277,6 +277,9 @@ class Collector:\n             if self.can_fast_delete(instance):\n                 with transaction.mark_for_rollback_on_error():\n                     count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n+                # Clear the PK on the in-memory instance to reflect deletion,\n+                # matching Django's behavior for other delete paths.\n+                setattr(instance, model._meta.pk.attname, None)\n                 return count, {model._meta.label: count}\n \n         with transaction.atomic(using=self.using, savepoint=False):\n"
  },
  "django__django-16662": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16662",
    "model_patch": "diff --git a/django/db/migrations/writer.py b/django/db/migrations/writer.py\nindex a59f0c8dcb..4fe7db9058 100644\n--- a/django/db/migrations/writer.py\n+++ b/django/db/migrations/writer.py\n@@ -173,9 +173,14 @@ class MigrationWriter:\n         else:\n             imports.add(\"from django.db import migrations\")\n \n-        # Sort imports by the package / module to be imported (the part after\n-        # \"from\" in \"from ... import ...\" or after \"import\" in \"import ...\").\n-        sorted_imports = sorted(imports, key=lambda i: i.split()[1])\n+        # Sort imports so that plain \"import module\" statements come before\n+        # \"from module import ...\" statements, and otherwise sort by module name.\n+        def import_sort_key(i):\n+            # kind: 0 for plain 'import module', 1 for 'from module import ...'\n+            kind = 1 if i.startswith(\"from \") else 0\n+            module = i.split()[1]\n+            return (kind, module)\n+        sorted_imports = sorted(imports, key=import_sort_key)\n         items[\"imports\"] = \"\\n\".join(sorted_imports) + \"\\n\" if imports else \"\"\n         if migration_imports:\n             items[\"imports\"] += (\n"
  },
  "django__django-13810": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13810",
    "model_patch": "diff --git a/django/core/handlers/base.py b/django/core/handlers/base.py\nindex 8e8c3da06a..2ae6a31c33 100644\n--- a/django/core/handlers/base.py\n+++ b/django/core/handlers/base.py\n@@ -50,12 +50,14 @@ class BaseHandler:\n             else:\n                 middleware_is_async = middleware_can_async\n             try:\n-                # Adapt handler, if needed.\n-                handler = self.adapt_method_mode(\n+                # Adapt handler, if needed, but don't mutate the current handler until\n+                # the middleware factory has been called successfully. This prevents\n+                # MiddlewareNotUsed from \"poisoning\" the handler for subsequent middlewares.\n+                adapted_handler = self.adapt_method_mode(\n                     middleware_is_async, handler, handler_is_async,\n                     debug=settings.DEBUG, name='middleware %s' % middleware_path,\n                 )\n-                mw_instance = middleware(handler)\n+                mw_instance = middleware(adapted_handler)\n             except MiddlewareNotUsed as exc:\n                 if settings.DEBUG:\n                     if str(exc):\n@@ -85,9 +87,10 @@ class BaseHandler:\n                     self.adapt_method_mode(False, mw_instance.process_exception),\n                 )\n \n+            # Only now update the handler and handler_is_async after successful\n+            # middleware instantiation.\n             handler = convert_exception_to_response(mw_instance)\n             handler_is_async = middleware_is_async\n-\n         # Adapt the top of the stack, if needed.\n         handler = self.adapt_method_mode(is_async, handler, handler_is_async)\n         # We only assign to this when initialization is complete as it is used\n"
  },
  "django__django-14011": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14011",
    "model_patch": "diff --git a/django/core/servers/basehttp.py b/django/core/servers/basehttp.py\nindex 14dab517c9..d8b07040a4 100644\n--- a/django/core/servers/basehttp.py\n+++ b/django/core/servers/basehttp.py\n@@ -17,6 +17,7 @@ from django.core.exceptions import ImproperlyConfigured\n from django.core.handlers.wsgi import LimitedStream\n from django.core.wsgi import get_wsgi_application\n from django.utils.module_loading import import_string\n+from django.db import connections\n \n __all__ = ('WSGIServer', 'WSGIRequestHandler')\n \n@@ -193,7 +194,16 @@ class WSGIRequestHandler(simple_server.WSGIRequestHandler):\n             self.rfile, self.wfile, self.get_stderr(), self.get_environ()\n         )\n         handler.request_handler = self      # backpointer for logging & connection closing\n-        handler.run(self.server.get_app())\n+        try:\n+            handler.run(self.server.get_app())\n+        finally:\n+            # Ensure database connections are closed in this thread after handling\n+            # the request to avoid dangling connections that may prevent the\n+            # test database from being torn down.\n+            try:\n+                connections.close_all()\n+            except Exception:\n+                pass\n \n \n def run(addr, port, wsgi_handler, ipv6=False, threading=False, server_cls=WSGIServer):\n"
  },
  "django__django-16631": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16631",
    "model_patch": "diff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 41387886c0..76173343d8 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -2301,19 +2301,24 @@ Default: ``[]``\n A list of fallback secret keys for a particular Django installation. These are\n used to allow rotation of the ``SECRET_KEY``.\n \n-In order to rotate your secret keys, set a new ``SECRET_KEY`` and move the\n-previous value to the beginning of ``SECRET_KEY_FALLBACKS``. Then remove the\n-old values from the end of the ``SECRET_KEY_FALLBACKS`` when you are ready to\n-expire the sessions, password reset tokens, and so on, that make use of them.\n-\n-.. note::\n-\n-    Signing operations are computationally expensive. Having multiple old key\n-    values in ``SECRET_KEY_FALLBACKS`` adds additional overhead to all checks\n-    that don't match an earlier key.\n-\n-    As such, fallback values should be removed after an appropriate period,\n-    allowing for key rotation.\n+When rotating your secret key, you can set a new ``SECRET_KEY`` and move the\n+previous value to the beginning of ``SECRET_KEY_FALLBACKS`` to allow signing\n+operations that consult fallback keys to verify values created with older\n+keys. For example, the cookie signer (used for signed cookies) and\n+:func:`~django.core.signing.loads`/ :func:`~django.core.signing.dumps` (when\n+passed ``fallback_keys``) will try the values in ``SECRET_KEY_FALLBACKS`` when\n+verifying signatures.\n+\n+Note, however, that not all uses of the secret key consult\n+``SECRET_KEY_FALLBACKS``. In particular,\n+:meth:`~django.contrib.auth.models.AbstractBaseUser.get_session_auth_hash()`\n+calls :func:`~django.utils.crypto.salted_hmac()` without providing an explicit\n+``secret`` argument, which causes it to use ``settings.SECRET_KEY`` only.\n+Putting an old key into ``SECRET_KEY_FALLBACKS`` will not preserve session\n+authentication hashes created by that method; rotating ``SECRET_KEY`` can\n+therefore log users out. Remove old values from the end of\n+``SECRET_KEY_FALLBACKS`` when you are ready to expire the signed values that\n+do make use of them.\n \n Uses of the secret key values shouldn't assume that they are text or bytes.\n Every use should go through :func:`~django.utils.encoding.force_str` or\n"
  },
  "astropy__astropy-7166": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "astropy__astropy-7166",
    "model_patch": "diff --git a/astropy/utils/misc.py b/astropy/utils/misc.py\nindex de3fb01d1..eab563041 100644\n--- a/astropy/utils/misc.py\n+++ b/astropy/utils/misc.py\n@@ -528,13 +528,50 @@ class InheritDocstrings(type):\n                 not key.startswith('_'))\n \n         for key, val in dct.items():\n-            if (inspect.isfunction(val) and\n-                is_public_member(key) and\n-                val.__doc__ is None):\n+            if not is_public_member(key):\n+                continue\n+\n+            # Handle plain functions, staticmethod/classmethod wrappers, and\n+            # properties. For each, determine whether the object defined in\n+            # this class is missing a docstring and if so, try to inherit it\n+            # from the first base class that provides it.\n+            if inspect.isfunction(val):\n+                needs_inherit = (val.__doc__ is None)\n+                def _set_doc(doc):\n+                    if doc is not None:\n+                        val.__doc__ = doc\n+            elif isinstance(val, (staticmethod, classmethod)):\n+                func = val.__func__\n+                needs_inherit = (func.__doc__ is None)\n+                def _set_doc(doc):\n+                    if doc is not None:\n+                        func.__doc__ = doc\n+            elif isinstance(val, property):\n+                # For properties, inherit the fget docstring (property.__doc__\n+                # typically proxies to fget.__doc__). If no fget, nothing to do.\n+                fget = val.fget\n+                if fget is None:\n+                    continue\n+                needs_inherit = (fget.__doc__ is None)\n+                def _set_doc(doc):\n+                    if doc is not None:\n+                        fget.__doc__ = doc\n+            else:\n+                continue\n+\n+            if needs_inherit:\n                 for base in cls.__mro__[1:]:\n-                    super_method = getattr(base, key, None)\n-                    if super_method is not None:\n-                        val.__doc__ = super_method.__doc__\n+                    super_attr = getattr(base, key, None)\n+                    if super_attr is None:\n+                        continue\n+                    # Unwrap descriptors on the base class to get at the\n+                    # underlying docstring.\n+                    if isinstance(super_attr, (staticmethod, classmethod)):\n+                        doc = getattr(super_attr.__func__, '__doc__', None)\n+                    else:\n+                        doc = getattr(super_attr, '__doc__', None)\n+                    if doc:\n+                        _set_doc(doc)\n                         break\n \n         super().__init__(name, bases, dct)\n"
  },
  "psf__requests-2931": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-2931",
    "model_patch": "diff --git a/requests/utils.py b/requests/utils.py\nindex 132cd2b..583357f 100644\n--- a/requests/utils.py\n+++ b/requests/utils.py\n@@ -691,7 +691,10 @@ def to_native_string(string, encoding='ascii'):\n     \"\"\"\n     out = None\n \n-    if isinstance(string, builtin_str):\n+    # Preserve bytes objects (important for binary payloads).\n+    if isinstance(string, bytes):\n+        out = string\n+    elif isinstance(string, builtin_str):\n         out = string\n     else:\n         if is_py2:\n"
  },
  "django__django-14053": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14053",
    "model_patch": "diff --git a/django/contrib/staticfiles/storage.py b/django/contrib/staticfiles/storage.py\nindex 494890cc23..f33b0a0618 100644\n--- a/django/contrib/staticfiles/storage.py\n+++ b/django/contrib/staticfiles/storage.py\n@@ -226,9 +226,17 @@ class HashedFilesMixin:\n             path for path in paths\n             if matches_patterns(path, self._patterns)\n         ]\n+\n+        # Track which original names we've already yielded to avoid\n+        # reporting the same file multiple times across multiple passes.\n+        yielded_names = set()\n+\n         # Do a single pass first. Post-process all files once, then repeat for\n-        # adjustable files.\n+        # adjustable files. Only yield each original name once.\n         for name, hashed_name, processed, _ in self._post_process(paths, adjustable_paths, hashed_files):\n+            if name in yielded_names:\n+                continue\n+            yielded_names.add(name)\n             yield name, hashed_name, processed\n \n         paths = {path: paths[path] for path in adjustable_paths}\n@@ -236,8 +244,13 @@ class HashedFilesMixin:\n         for i in range(self.max_post_process_passes):\n             substitutions = False\n             for name, hashed_name, processed, subst in self._post_process(paths, adjustable_paths, hashed_files):\n-                yield name, hashed_name, processed\n+                if name in yielded_names:\n+                    # Already reported this original file; skip duplicate yield.\n+                    substitutions = substitutions or subst\n+                    continue\n+                yielded_names.add(name)\n                 substitutions = substitutions or subst\n+                yield name, hashed_name, processed\n \n             if not substitutions:\n                 break\n"
  },
  "django__django-7530": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-7530",
    "model_patch": "diff --git a/django/core/management/commands/makemigrations.py b/django/core/management/commands/makemigrations.py\nindex 0b39536f5b..e648880ef0 100644\n--- a/django/core/management/commands/makemigrations.py\n+++ b/django/core/management/commands/makemigrations.py\n@@ -105,7 +105,7 @@ class Command(BaseCommand):\n                     # At least one model must be migrated to the database.\n                     router.allow_migrate(connection.alias, app_label, model_name=model._meta.object_name)\n                     for app_label in consistency_check_labels\n-                    for model in apps.get_models(app_label)\n+                    for model in apps.get_app_config(app_label).get_models()\n             )):\n                 loader.check_consistent_history(connection)\n \n"
  },
  "django__django-13297": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13297",
    "model_patch": "diff --git a/django/views/generic/base.py b/django/views/generic/base.py\nindex ea5baca08d..5a6eb0a553 100644\n--- a/django/views/generic/base.py\n+++ b/django/views/generic/base.py\n@@ -23,10 +23,40 @@ class ContextMixin:\n     \"\"\"\n     extra_context = None\n \n-    def get_context_data(self, **kwargs):\n+        def get_context_data(self, **kwargs):\n         kwargs.setdefault('view', self)\n         if self.extra_context is not None:\n             kwargs.update(self.extra_context)\n+        # Resolve any lazy objects (e.g. SimpleLazyObject or lazy() proxies)\n+        # so consumers of the context get concrete values. This avoids cases\n+        # where code expects a real value (like a slug string) but receives\n+        # a SimpleLazyObject that can break lookups or DB bindings.\n+        try:\n+            from django.utils.functional import Promise, LazyObject\n+        except Exception:\n+            Promise = LazyObject = None\n+\n+        for key, value in list(kwargs.items()):\n+            unwrapped = value\n+            try:\n+                if LazyObject is not None and isinstance(value, LazyObject):\n+                    # Accessing __class__ on the proxy will force evaluation.\n+                    _ = value.__class__\n+                    unwrapped = value\n+                elif Promise is not None and isinstance(value, Promise):\n+                    # For lazy() proxies (Promise subclasses), try to force\n+                    # evaluation. bool() will often trigger the proxy; fallback\n+                    # to str() if needed.\n+                    try:\n+                        bool(value)\n+                        unwrapped = value\n+                    except Exception:\n+                        unwrapped = str(value)\n+            except Exception:\n+                # If forcing evaluation raises, leave original value.\n+                unwrapped = value\n+\n+            kwargs[key] = unwrapped\n         return kwargs\n \n \n"
  },
  "scikit-learn__scikit-learn-26323": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "scikit-learn__scikit-learn-26323",
    "model_patch": "diff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py\nindex 4eddd12b1..cd8e8bde2 100644\n--- a/sklearn/compose/_column_transformer.py\n+++ b/sklearn/compose/_column_transformer.py\n@@ -293,31 +293,26 @@ class ColumnTransformer(TransformerMixin, _BaseComposition):\n             Estimator instance.\n         \"\"\"\n         super().set_output(transform=transform)\n+\n+        # Set output on the transformers provided in the transformers list and\n+        # on any fitted transformers in transformers_.\n         transformers = (\n             trans\n             for _, trans, _ in chain(\n-                self.transformers, getattr(self, \"transformers_\", [])\n+                self.transformers, getattr(self, \"transformers_,\", [])\n             )\n             if trans not in {\"passthrough\", \"drop\"}\n         )\n         for trans in transformers:\n             _safe_set_output(trans, transform=transform)\n \n-        return self\n-\n-    def get_params(self, deep=True):\n-        \"\"\"Get parameters for this estimator.\n-\n-        Returns the parameters given in the constructor as well as the\n-        estimators contained within the `transformers` of the\n-        `ColumnTransformer`.\n-\n-        Parameters\n-        ----------\n-        deep : bool, default=True\n-            If True, will return the parameters for this estimator and\n-            contained subobjects that are estimators.\n+        # Also ensure we set the output on the (unfitted) remainder estimator\n+        # when it is an estimator (i.e. not drop or passthrough).\n+        # The fitted remainder (if any) will already be part of transformers_.\n+        if hasattr(self, \"remainder\") and self.remainder not in {\"passthrough\", \"drop\"}:\n+            _safe_set_output(self.remainder, transform=transform)\n \n+        return self\n         Returns\n         -------\n         params : dict\n"
  },
  "psf__requests-1921": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-1921",
    "model_patch": "diff --git a/requests/structures.py b/requests/structures.py\nindex a175913..181679f 100644\n--- a/requests/structures.py\n+++ b/requests/structures.py\n@@ -69,6 +69,17 @@ class CaseInsensitiveDict(collections.MutableMapping):\n         self.update(data, **kwargs)\n \n     def __setitem__(self, key, value):\n+        # If a value of None is provided, treat this as deletion of the key.\n+        # This mirrors typical dict behavior (and aligns with user expectations\n+        # when removing session default headers by setting to None).\n+        if value is None:\n+            try:\n+                del self._store[key.lower()]\n+            except KeyError:\n+                # If the key does not exist, nothing to delete.\n+                pass\n+            return\n+\n         # Use the lowercased key for lookups, but store the actual\n         # key alongside the value.\n         self._store[key.lower()] = (key, value)\n"
  },
  "django__django-12406": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-12406",
    "model_patch": "Avoid explicit empty choice (\"---------\") for required ModelChoiceField when using RadioSelect (no initial); updated django/forms/models.py\n\nNo changes detected to generate a patch."
  },
  "django__django-10973": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-10973",
    "model_patch": "diff --git a/django/db/backends/postgresql/client.py b/django/db/backends/postgresql/client.py\nindex 7fca6eff30..1687b6e9be 100644\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -58,7 +58,22 @@ class DatabaseClient(BaseDatabaseClient):\n                     pass\n             # Allow SIGINT to pass to psql to abort queries.\n             signal.signal(signal.SIGINT, signal.SIG_IGN)\n-            subprocess.check_call(args)\n+            # Prefer passing the password via PGPASSWORD in the environment\n+            # to avoid creating a temporary .pgpass file. Fall back to the\n+            # .pgpass file if setting the environment would fail.\n+            try:\n+                env = None\n+                if passwd:\n+                    env = os.environ.copy()\n+                    env['PGPASSWORD'] = passwd\n+                if env is not None:\n+                    subprocess.run(args, env=env, check=True)\n+                else:\n+                    subprocess.run(args, check=True)\n+            except UnicodeEncodeError:\n+                # If setting env failed due to encoding issues, fall back to\n+                # creating a temporary .pgpass file and invoking psql.\n+                subprocess.check_call(args)\n         finally:\n             # Restore the original SIGINT handler.\n             signal.signal(signal.SIGINT, sigint_handler)\n"
  },
  "sympy__sympy-24213": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sympy__sympy-24213",
    "model_patch": "diff --git a/sympy/physics/units/unitsystem.py b/sympy/physics/units/unitsystem.py\nindex a436880c4b..5705c821c2 100644\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -175,7 +175,7 @@ def _collect_factor_and_dimension(self, expr):\n             for addend in expr.args[1:]:\n                 addend_factor, addend_dim = \\\n                     self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n+                if not self.get_dimension_system().equivalent_dims(dim, addend_dim):\n                     raise ValueError(\n                         'Dimension of \"{}\" is {}, '\n                         'but it should be {}'.format(\n"
  },
  "pytest-dev__pytest-7490": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "pytest-dev__pytest-7490",
    "model_patch": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..c2adf9559 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -261,7 +261,17 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n+\n+    # Retrieve previously stored xfail evaluation (if any)\n     xfailed = item._store.get(xfailed_key, None)\n+    # Re-evaluate xfail marks to catch any markers that may have been added\n+    # dynamically during test execution (e.g. via request.node.add_marker).\n+    # If the evaluation changed, update the stored value so the dynamically\n+    # added xfail is respected in reporting.\n+    current_xfailed = evaluate_xfail_marks(item)\n+    if current_xfailed != xfailed:\n+        item._store[xfailed_key] = xfailed = current_xfailed\n+\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\n"
  },
  "sphinx-doc__sphinx-9658": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9658",
    "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex a255c0691..67a5621f8 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -1664,7 +1664,20 @@ class ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type:\n             self.env.events.emit('autodoc-process-bases',\n                                  self.fullname, self.object, self.options, bases)\n \n-            base_classes = [restify(cls) for cls in bases]\n+            base_classes = []\n+            for cls in bases:\n+                try:\n+                    # If the base is a mocked object, prefer its display name which\n+                    # contains \"module.ClassName\" (see sphinx.ext.autodoc.mock._make_subclass).\n+                    if ismock(cls):\n+                        display = getattr(cls, '__display_name__', None)\n+                        if display:\n+                            base_classes.append(':py:class:`%s`' % display)\n+                            continue\n+                except Exception:\n+                    # fall back to normal restify on any error\n+                    pass\n+                base_classes.append(restify(cls))\n             sourcename = self.get_sourcename()\n             self.add_line('', sourcename)\n             self.add_line('   ' + _('Bases: %s') % ', '.join(base_classes), sourcename)\n"
  },
  "sphinx-doc__sphinx-9230": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9230",
    "model_patch": "diff --git a/sphinx/ext/napoleon/docstring.py b/sphinx/ext/napoleon/docstring.py\nindex d8cb75a5f..b57061ed0 100644\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -928,6 +928,42 @@ def _recombine_set_tokens(tokens: List[str]) -> List[str]:\n     return list(combine_set(token_queue))\n \n \n+def _recombine_paren_tokens(tokens: List[str]) -> List[str]:\n+    \"\"\"\n+    Recombine tokens that were split inside parentheses into a single token.\n+    Keeps constructs like \"dict(str, str)\" as one token instead of being split\n+    at the comma by the general tokenizer.\n+    \"\"\"\n+    token_queue = collections.deque(tokens)\n+\n+    def combine_paren(tokens):\n+        while True:\n+            try:\n+                token = tokens.popleft()\n+            except IndexError:\n+                break\n+\n+            if token == \"(\":\n+                # Collect tokens until the matching closing parenthesis, respecting nesting.\n+                parts = [\"(\"]\n+                open_paren = 1\n+                while tokens:\n+                    t = tokens.popleft()\n+                    parts.append(t)\n+                    if t == \"(\":\n+                        open_paren += 1\n+                    elif t == \")\":\n+                        open_paren -= 1\n+                        if open_paren == 0:\n+                            break\n+                # Join collected parts into a single token, e.g. \"(str, str)\"\n+                yield \"\".join(parts)\n+            else:\n+                yield token\n+\n+    return list(combine_paren(token_queue))\n+\n+\n def _tokenize_type_spec(spec: str) -> List[str]:\n     def postprocess(item):\n         if _default_regex.match(item):\n@@ -1026,6 +1062,9 @@ def _convert_numpy_type_spec(_type: str, location: str = None, translations: dic\n \n     tokens = _tokenize_type_spec(_type)\n     combined_tokens = _recombine_set_tokens(tokens)\n+    # also recombine parentheses so constructs like dict(str, str)\n+    # are kept together instead of being split at commas\n+    combined_tokens = _recombine_paren_tokens(combined_tokens)\n     types = [\n         (token, _token_type(token, location))\n         for token in combined_tokens\n"
  },
  "sphinx-doc__sphinx-7590": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7590",
    "model_patch": "diff --git a/sphinx/domains/cpp.py b/sphinx/domains/cpp.py\nindex fe52d881d..5e73c7fa0 100644\n--- a/sphinx/domains/cpp.py\n+++ b/sphinx/domains/cpp.py\n@@ -855,6 +855,29 @@ class ASTStringLiteral(ASTLiteral):\n         signode.append(nodes.Text(txt, txt))\n \n \n+\n+\n+class ASTUserDefinedLiteral(ASTLiteral):\n+    \"\"\"Represents a user-defined literal: a literal (number/string/char) followed\n+    immediately by an identifier suffix, e.g. 42_km, \"hello\"_s, 'c'_x.\"\"\"\n+    def __init__(self, literal_text: str, suffix: str) -> None:\n+        # `literal_text` should be the literal text including quotes for strings/chars,\n+        # or the numeric literal text for numbers. `suffix` is the identifier.\n+        self.literal_text = literal_text\n+        self.suffix = suffix\n+\n+    def _stringify(self, transform: StringifyTransform) -> str:\n+        return self.literal_text + self.suffix\n+\n+    def get_id(self, version: int) -> str:\n+        # A simple id representation for UDLs. Not intended to be a perfect mangling.\n+        return \"LUDL_%s_%s\" % (self.literal_text, self.suffix)\n+\n+    def describe_signature(self, signode: TextElement, mode: str,\n+                           env: \"BuildEnvironment\", symbol: \"Symbol\") -> None:\n+        txt = str(self)\n+        signode.append(nodes.Text(txt, txt))\n+\n class ASTCharLiteral(ASTLiteral):\n     def __init__(self, prefix: str, data: str) -> None:\n         self.prefix = prefix  # may be None when no prefix\n@@ -4643,6 +4666,7 @@ class DefinitionParser(BaseParser):\n             self.pos += 1\n         return self.definition[startPos:self.pos]\n \n+    \n     def _parse_literal(self) -> ASTLiteral:\n         # -> integer-literal\n         #  | character-literal\n@@ -4662,12 +4686,27 @@ class DefinitionParser(BaseParser):\n                       integer_literal_re, octal_literal_re]:\n             pos = self.pos\n             if self.match(regex):\n+                # consume standard suffix characters first (u, U, l, L, f, F)\n                 while self.current_char in 'uUlLfF':\n                     self.pos += 1\n+                # If an identifier immediately follows the literal, treat this\n+                # as a user-defined literal suffix (no whitespace allowed).\n+                # Try matching an identifier at the current position.\n+                prev_pos = self.pos\n+                if self.match(identifier_re):\n+                    suffix = self.matched_text\n+                    literal_text = self.definition[pos:prev_pos]\n+                    return ASTUserDefinedLiteral(literal_text, suffix)\n                 return ASTNumberLiteral(self.definition[pos:self.pos])\n \n         string = self._parse_string()\n         if string is not None:\n+            # After a string literal, a user-defined literal suffix may follow\n+            # immediately (no whitespace). Try matching an identifier at current\n+            # position.\n+            if self.match(identifier_re):\n+                suffix = self.matched_text\n+                return ASTUserDefinedLiteral(string, suffix)\n             return ASTStringLiteral(string)\n \n         # character-literal\n@@ -4675,6 +4714,16 @@ class DefinitionParser(BaseParser):\n             prefix = self.last_match.group(1)  # may be None when no prefix\n             data = self.last_match.group(2)\n             try:\n+                # After a character literal, a user-defined literal suffix may\n+                # follow immediately (no whitespace). Try matching an identifier.\n+                if self.match(identifier_re):\n+                    suffix = self.matched_text\n+                    # rebuild the char literal text (including prefix if present)\n+                    if prefix:\n+                        lit_text = prefix + \"'\" + data + \"'\"\n+                    else:\n+                        lit_text = \"'\" + data + \"'\"\n+                    return ASTUserDefinedLiteral(lit_text, suffix)\n                 return ASTCharLiteral(prefix, data)\n             except UnicodeDecodeError as e:\n                 self.fail(\"Can not handle character literal. Internal error was: %s\" % e)\n@@ -4682,7 +4731,7 @@ class DefinitionParser(BaseParser):\n                 self.fail(\"Can not handle character literal\"\n                           \" resulting in multiple decoded characters.\")\n \n-        # TODO: user-defined lit\n+        # No literal found\n         return None\n \n     def _parse_fold_or_paren_expression(self) -> ASTExpression:\n"
  },
  "sympy__sympy-17655": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sympy__sympy-17655",
    "model_patch": "Import Mul and unwrap scalar*Point in Point.__add__ so number*Point works like Point*number\n----END_FUNCTION_CALL****\n\nNo changes detected to generate a patch."
  }
}